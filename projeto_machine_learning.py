# -*- coding: utf-8 -*-
"""Projeto Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11e85McKTQWqLzprzUyFJq9xe2roaylq0
"""

#Projeto: Classificar músicas agitadas e lentas do Spotify
import pandas as pd

pd.set_option("display.max_columns",None)
df=pd.read_csv("/dataset (1).csv")
df.head() #Mostrar primeiras 5 linhas

df.shape #linhas / colunas



"""# Criar a variável Target

De acordo com a regra de negócio, a coluna Valence é uma medida de 0,0 a 1,0 que descrever a positividade musical transmitida por uma faixa. Faixas com alta valência soam mais postivas , enquanto faixas de alta valência, soam mais negativas. Para criar a nossa coluna coluna. Para criar a nossa coluna alvo do modelo preditivo, vamos utilizar a coluna **"valence"** para ser nosso critério de músicas agitadas ou lentas.

"""

import matplotlib.pyplot as plt
#Analisando a coluna "valence" para criar a nossa target
#Vou plotar um gráfico de colunas(historgrama), utilizando o matlib, para fazer uma análise da nossa coluna
plt.hist(df["valence"], bins= 20 , color= "blue"  , edgecolor = "black" )
#bin (barras) , edge color (cor das bordas)
plt.xlabel("Valance") #Eixo X
plt.ylabel("Frequência") #Eixo Y / O quanto essa variável se repete (Quantidade) , normalmente usado com histogramas
plt.title("Histrograma da Coluna Valence") #Título
plt.show() #Executar gráfico

df["valence"].describe # Descrever como se comporta meu histograma

def categorizar_valence (row):
  if row["valence"] >= 0.5:
    return "Agitada"
  else:
    return "Lenta"
#Criando uma nova coluna target usando a  função categorizar_valence
df["target"] = df.apply(categorizar_valence, axis=1)
df.head()

"""# Feature Engineering
Como próximo passo, vamos armazenar um novo dataframe apenas as colunas necessárias para nossa classificação de músicas.

"""

df.columns #Mostrar colunas
df_musicas = df.drop(["Unnamed: 0" , "track_id"], axis = 1) #Excluir coluna
df_musicas.head() #Mostrar

"""# Tratando Dados Categóricos
LabelEncoder: Essa classe é utilizada para codificar rótulos de classes em números inteiros. É frequentemente usado quando se trabalha com algoritmos de aprendizado supervisionados que requerem rótulos númericos.Basicamente vou transformar strings em número, para uma melhor análise de ML, pois ele não entende textos .Vou criar um número index, para uma banda específica, por exemplo.



"""

def label_encoder_dataframe (df, columns_to_encode):
  from sklearn.preprocessing import LabelEncoder
  le = LabelEncoder()

  for column in columns_to_encode:
    if column in df.columns:
      df[column] = le.fit_transform(df[column])
  else:
      print("A lista possui colunas que não existem no DataFrame.")
  return df

colunas_a_codificar = ["artists", "album_name", "track_name", "target","explicit","track_genre"]
label_encoder_dataframe(df_musicas, colunas_a_codificar)
df_musicas.head()
#Todos valores de textos, transformei em número (index), como o LabelEncoder, para conseguir fazer modelos utilizando o Machine Learning

"""# Analisando as colunas que vão compor nosso modelo
Usar seaborn para criar uma matriz coloridade e colocar intesidade de cores, porém a lib pandas também faz a ação, utilizando o .core

A matriz de correlação pode ser usada para determinar quais variáveis estão significativamente conectadas entre si e quais são pouco ou nada correlacionadas. Essas informações podem ser usadas para criar previsões e julgamentos informados com base nos dados.
"""

import seaborn  as sns
import pandas as pd

correlation_matrix = df_musicas.corr().round(2) #Criação da matriz de correlação

fig, ax = plt.subplots(figsize = ((12,10)))
sns.heatmap(correlation_matrix, annot = True ,linewidths= 5 , ax=ax , cmap="coolwarm") #annot=True faz com que os valores de correlação sejam exibidos dentro de cada célula do mapa de calor.

"""# Entendendo o equilíbrio da Target

"""

#Verificando o equilíbro das classes
round (df_musicas["target"].value_counts(normalize= True)*100,2)
#Na executação, ( 0=  Agitada , 1= Lenta , conseguindo ver isso, executando o comando set ).

set (df["target"])

set (df_musicas["target"]) # No código original, aparece a string, já no código df_musicas, aparece o número, criado pelo label encoder

"""Separando os dados em treino e teste .

"""

df_musicas.columns

#Separando os dados em treino e teste (método hold out)
from sklearn.model_selection import train_test_split
#X = df_musicas.drop("target", axis = 1)
X = df_musicas[["popularity", "duration_ms", "danceability", "energy",  "loudness", "acousticness","instrumentalness","liveness","track_genre"]]

y = df_musicas["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify = y, random_state=7) #teste size = 20% teste , random state é uma semente aleatória .

print(X_train.shape, X_test.shape)

"""  Normalizando os Dados

  Ao aplicar o MinMaxScaler, todos os valores dos dados serão transformados para o internvalo de 0 e 1, onde o valor mínimo será de 0 e o valor máximo será 1. Essa técnica é especialmente útil quando os algorítmos de aprendizado de máquina são sensíveis a escala dos dados.

  Essa normalização é aplicada antes de dividir em conjuntos de teste no conjunto de treino. Isso poder levar a uma avaliação otimista do desempenho do modelo, uma vez que o modelo terá visto parte dos dados de teste durante o treinamento. Este tipo de problema também é chamado de **Data Leak (Vazamento de Dados)**
"""

from sklearn.preprocessing import MinMaxScaler
# Criar uma instância do MinMaxScaler
scaler =  MinMaxScaler()

scaler.fit(X_train) #Ajustar o scaler aos dados de treinamento

X_train_escalonado = scaler.transform(X_train)
X_test_escalonado = scaler.transform(X_test)

"""# Criando uma função para executar modelos de Machine Learning

Vamos criar uma função que vai executar o modelo preditivo e avaliar sua performance utilizando as principais métricas de validação


"""

def roda_modelo(modelo):

    from sklearn.metrics import roc_curve, roc_auc_score, classification_report

    # Treinando modelo com os dados de treino
    modelo.fit(X_train_escalonado , y_train)

    # Calculando a probabilidade e calculando o AUC
    prob_predic = modelo.predict_proba(X_test_escalonado) # obter as probabilidades associadas às classes previstas para cada instância de dados
    auc = roc_auc_score(y_test, prob_predic[:,1])
    print(f"AUC {auc}")

    # Fazendo a predicao dos dados de teste e calculando o classification report
    predicao = modelo.predict(X_test_escalonado)
    print("\nClassification Report")
    print(classification_report(y_test, predicao))

    print("\nRoc Curve\n")
    # Fazer previsões de probabilidades
    y_pred_probs = modelo.predict_proba(X_test_escalonado)[:, 1]

    # Calcular a curva ROC
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)

    # Calcular a AUC (área sob a curva ROC)
    auc = roc_auc_score(y_test, y_pred_probs)

    # Plotar a curva ROC
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc:.2f})') # linewidth
    plt.plot([0, 1], [0, 1], color='gray',linestyle='--')
    plt.xlabel('Taxa de Falso Positivo')
    plt.ylabel('Taxa de Verdadeiro Positivo')
    plt.title('Curva ROC')
    plt.legend(loc='lower right')
    plt.show()

    # Converter probabilidades em classes preditas (0 ou 1)
    y_pred = (y_pred_probs > 0.5).astype(int)

"""# Regressão Logística
O modelo se baseia em uma função logística, que transforma as variáveis independentes em uma probabilidade entre 0 e 1. Para novas entradas de dados, o modelo calcula a probabilidade do evento binário acontecer.


"""

from sklearn.linear_model import LogisticRegression
modelo_logistico = LogisticRegression()
roda_modelo (modelo_logistico)

"""# KNN (K-Nearest Neighbors)
Para um novo ponto de dados, o KNN identifica os K pontos maios próximos (vizinhos) no conjunto de treinamento. A classe do novo ponto é a classe mais frequente entre os K vizinhos.

"""

from sklearn.neighbors import KNeighborsClassifier
modelo_knn = KNeighborsClassifier(n_neighbors = 3)
roda_modelo(modelo_knn)

"""# Random Forest
O Random Forest é um algoritmo de ensemble learning que combina vários modelos para melhorar performance. O modelo cria uma floresta de árvores de decisão, onde cada árvore é treinada em um subconjunto aleatório dos dados (bootstrap). A classe final do novo ponto de dados é a classe mais votada pelas árvores da floresta.

"""

from sklearn.ensemble import RandomForestClassifier
modelo_random_forest = RandomForestClassifier(n_estimators = 100)
roda_modelo(modelo_random_forest)

"""#Testando novos parâmetros com Grid Search"""

from sklearn.model_selection import GridSearchCV

#Defina os parâmetros a serem testados
param_grid = {
    "n_estimators" : [100,200,300],
    "max_depth": [5,10,15] }
#GridSearchCV
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5 , scoring = "f1", n_jobs = 1)

#Ajuste o modelo ao conjunto de dados
grid_search.fit(X_train_escalonado, y_train)


rf_params = grid_search.best_params_
print("Melhores hiperparâmetros: ", rf_params)

"""Testando com os melhores parametros encontrados

"""

from sklearn.ensemble import RandomForestClassifier
modelo_random_forest_gs = RandomForestClassifier(n_estimators = 300, max_depth = 15)
roda_modelo(modelo_random_forest)

"""# TESTANDO O MODELO"""

import numpy as np

novos_dados = pd.read_excel("/novos_dados (1).xlsx")
base_original = pd.read_excel("/novos_dados (1).xlsx")

#Criando a pipeline
coluna = ['track_genre']
label_encoder_dataframe(novos_dados, coluna)
novos_dados = scaler.transform(novos_dados)

# Fit the model to your training data (assuming you have X_train_escalonado and y_train available)
modelo_random_forest_gs.fit(X_train_escalonado, y_train) # Add this line to train the model

# Realize a previsão usando o modelo Random Forest treinado
previsoes = modelo_random_forest_gs.predict(novos_dados)

# Obtendo o predict
def mapear_valor(valores):
    resultados = []
    for valor in valores:
        if valor == 0:
            resultados.append('Música agitada')
        elif valor == 1:
            resultados.append('Música lenta')
        else:
            resultados.append('Desconhecido')
    return np.array(resultados)

base_original['target'] = mapear_valor(previsoes)
base_original.head()

# @title track_genre vs target

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['target'].value_counts()
    for x_label, grp in base_original.groupby('track_genre')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('track_genre')
_ = plt.ylabel('target')